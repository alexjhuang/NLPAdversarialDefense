


print('hello')





from transformers import BertForSequenceClassification, ElectraForSequenceClassification, BertTokenizer, ElectraTokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# Load dataset (IMDB for text classification as an example)
dataset = load_dataset("imdb")

# Load tokenizers for BERT and Electra
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
electra_tokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')

# Tokenize data
def tokenize_function(examples, tokenizer):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

bert_encoded = dataset.map(lambda x: tokenize_function(x, bert_tokenizer), batched=True)
electra_encoded = dataset.map(lambda x: tokenize_function(x, electra_tokenizer), batched=True)

# Load pre-trained models for classification
bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
electra_model = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator', num_labels=2)




